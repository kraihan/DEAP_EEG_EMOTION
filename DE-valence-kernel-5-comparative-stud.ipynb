{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9551391,"sourceType":"datasetVersion","datasetId":5819581}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nimport time\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split,KFold\nframeNum = 60 \n\n# load data\ndfs = []\nfor i in range(1,33):\n  for j in range(1,41):\n    filename = '/kaggle/input/deap-research/DE/subject_%d_trial_%d.txt'%(i,j)\n    cols = [i for i in range(frameNum)]\n    df = pd.read_csv(filename, header = None, usecols = cols, delimiter=',')   \n    dfs.append(df.values)\n    #print('participant%dvideo%d.txt'%(i,j))\n\ndfs = np.array(dfs)\nprint('dataLoaded:')\nprint(dfs.shape)\n\n#dataLoaded:\n#(1280, 1024, 60)\n\n# normalize\nx_min = dfs.min(axis = (1,2),keepdims=True)\nx_max = dfs.max(axis = (1,2),keepdims=True)\ndfs_normal = (dfs-x_min)/(x_max-x_min)\ndepth = 3\n# divide frames ,or 60s is too long for a single 3dinput\nreshape_dfs = np.split(dfs_normal, frameNum/depth, axis=2)\nreshape_dfs = np.array(reshape_dfs)\nreshape_dfs = np.reshape(reshape_dfs,[-1,1024,depth])\nprint(reshape_dfs.shape)\n\n#(25600, 1024, 3)\n\n# load label\ncols = ['valence', 'arousal', 'dominance', 'liking']\nlabel_df = pd.read_csv('/kaggle/input/deap-research/label.txt',\n    usecols = [i for i in range(4)], header=None, delimiter=',')\nprint(label_df.shape)\nlabel_df.columns = cols\nlabel_df[label_df<4.5] = 0\nlabel_df[label_df>=4.5] = 1\n\n#(1280, 4)\n\n# valence\nlabel = label_df['arousal'].astype(int).values\nlabel = np.tile(label,20)\nprint(label.shape)\n\nclass cnn_classifier(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.conv11 = nn.Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n    self.conv12 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n    self.pool1 = nn.MaxPool3d(kernel_size=2, padding=(0,0,1))\n    \n    self.conv21 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n    self.conv22 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n    self.pool2 = nn.MaxPool3d(kernel_size=2, padding=(0,0,1))\n    \n    self.conv31 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n    self.conv32 = nn.Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n    self.pool3 = nn.MaxPool3d(kernel_size=2, padding=0)\n    \n\n    self.fc_layer = nn.Linear(128*4*4*1, 2)\n    \n    self.dropout_layer = nn.Dropout(p=0.5)\n\n  def forward(self, xb):\n    h1 = self.conv11(xb)\n    h1 = self.conv12(h1)\n    h1 = self.dropout_layer(h1)\n    h1 = self.pool1(h1)\n    h1 = F.relu(h1)\n\n    h2 = self.conv21(h1)\n    h2 = self.conv22(h2)\n    #h2 = self.dropout_layer(h2)\n    h2 = self.pool2(h2)\n    h2 = F.relu(h2) \n\n    h3 = self.conv31(h2)\n    h3 = self.conv32(h3)\n    #h3 = self.dropout_layer(h3)\n    h3 = self.pool3(h3)\n    h3 = F.relu(h3) \n    \n    \n    # flatten the output from conv layers before feeind it to FC layer\n    flatten = h3.view(-1, 128*4*4*1)\n    out = self.fc_layer(flatten)\n    #out = self.dropout_layer(out)\n    return out\n\ndef train_model(model, x_train, y_train, x_test, y_test, epochs=50 , batch_size=32, lr=0.0001, weight_decay=0):\n  # data\n  train_dataset = TensorDataset(x_train, y_train)\n  train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n\n  # loss function\n  loss_func = F.cross_entropy\n\n  # optimizer\n  #optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n  # figure\n  train_a = list([])\n  test_a = list([])\n\n  # training loop\n  for epoch in range(epochs):\n    model.train()\n    tic = time.time()\n    acc_train = []\n    for xb, yb in train_data_loader:    \n      xb, yb = xb.to(device), yb.to(device)\n      pred = model(xb)\n      loss = loss_func(pred, yb)\n      \n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      acc_train.append(pred.detach().argmax(1).eq(yb).float().mean().cpu().numpy())\n    acc_train = np.mean(acc_train)\n    toc = time.time()\n    \n    with torch.no_grad():\n      model.eval()\n      y_pred = model(x_test.to(device))\n      acc = y_pred.argmax(1).eq(y_test.to(device)).float().mean().cpu().numpy()\n\n    train_a.append(acc_train)\n    test_a.append(acc)\n    print('Loss at epoch %d : %f, train_acc: %f, test_acc: %f, running time: %d'% (epoch, loss.item(), acc_train, acc, toc-tic))\n  \n  train_amax = max(train_a)\n  test_amax = max(test_a)\n\n  # draw an accuray figure\n  \"\"\"\n  plt.plot(train_a,'y.-.')\n  plt.plot(test_a,'.-.')\n  plt.xlabel('epoch')\n  plt.ylabel('accuracy')\"\"\"\n  plt.plot(train_a, 'y.-.', label='Train Accuracy')  # Add label for train accuracy\n  plt.plot(test_a, '.-.', label='Test Accuracy')    # Add label for test accuracy\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.legend()  # Enable legend\n  plt.title('Train vs Test Accuracy Over Epochs')  # Add title\n  #plt.show()\n\n\n\n  return train_amax,test_amax\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix\n\n# Tracking variables for best fold\nbest_fold_idx = -1\nbest_test_acc = -1\nbest_fold_train_acc = []\nbest_fold_test_acc = []\nbest_fold_losses = []\nbest_y_true = None\nbest_y_pred = None\nbest_y_scores = None\n\n# K-fold validation\nx_train = reshape_dfs\ny_train = label\nkf = KFold(n_splits=10)\n\nfold_idx = 0\nfor train_index, test_index in kf.split(x_train, y_train):  \n    print(f\"Fold {fold_idx}:\")\n    \n    # Splitting the data\n    x_train_fold = x_train[train_index]\n    y_train_fold = y_train[train_index]\n    x_test_fold = x_train[test_index]\n    y_test_fold = y_train[test_index]\n    \n    # Convert to tensors\n    x_train_fold = torch.from_numpy(x_train_fold).float()\n    y_train_fold = torch.from_numpy(y_train_fold).long()\n    x_test_fold = torch.from_numpy(x_test_fold).float()\n    y_test_fold = torch.from_numpy(y_test_fold).long()\n    \n    # Initialize model\n    model = cnn_classifier()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = model.to(device)\n\n    # Lists to store accuracies and losses for each fold\n    train_acc_per_fold = []\n    test_acc_per_fold = []\n    losses_per_fold = []\n\n    # Training loop\n    def train_model(model, x_train, y_train, x_test, y_test, epochs=50 , batch_size=32, lr=0.0001, weight_decay=0):\n        train_dataset = TensorDataset(x_train, y_train)\n        train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n\n        # Loss function\n        loss_func = F.cross_entropy\n        # Optimizer\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n        # Training loop\n        for epoch in range(epochs):\n            model.train()\n            toc = time.time()\n            acc_train = []\n            epoch_loss = 0\n            for xb, yb in train_data_loader:    \n                xb, yb = xb.to(device), yb.to(device)\n                pred = model(xb)\n                loss = loss_func(pred, yb)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                \n                acc_train.append(pred.detach().argmax(1).eq(yb).float().mean().cpu().numpy())\n                epoch_loss += loss.item()\n\n            acc_train = np.mean(acc_train)\n            tic = time.time()\n            train_acc_per_fold.append(acc_train)\n            losses_per_fold.append(epoch_loss / len(train_data_loader))  # Avg loss\n\n            # Evaluate on test set\n            model.eval()\n            with torch.no_grad():\n                y_pred = model(x_test.to(device)).argmax(1)\n                acc_test = y_pred.eq(y_test.to(device)).float().mean().cpu().numpy()\n                test_acc_per_fold.append(acc_test)\n                \n            print('Loss at epoch %d : %f, train_acc: %f, test_acc: %f, running time: %d'% (epoch, loss.item(), acc_train, acc_test, toc-tic))\n\n    # Train the model on the current fold\n    train_model(model, x_train_fold.view(-1, 1, 32, 32, depth), y_train_fold,\n                x_test_fold.view(-1, 1, 32, 32, depth), y_test_fold)\n    \n    # After training, check if this fold has the best test accuracy\n    if max(test_acc_per_fold) > best_test_acc:\n        best_test_acc = max(test_acc_per_fold)\n        best_fold_idx = fold_idx\n        best_fold_train_acc = train_acc_per_fold\n        best_fold_test_acc = test_acc_per_fold\n        best_fold_losses = losses_per_fold\n\n        # Save the best fold's predictions for further evaluation\n        best_y_true = y_test_fold.cpu().numpy()\n        best_y_pred = model(x_test_fold.view(-1, 1, 32, 32, depth).to(device)).argmax(dim=1).cpu().numpy()\n        best_y_scores = model(x_test_fold.view(-1, 1, 32, 32, depth).to(device)).softmax(dim=1)[:, 1].detach().cpu().numpy()\n\n    \n    fold_idx += 1\n\n# After k-fold, evaluate and plot results for the best fold\nprint(f\"Best Fold: {best_fold_idx}, Test Accuracy: {best_test_acc}\")\n\n# Plot training and test accuracy for the best fold\nplt.figure(figsize=(14, 6))\n\n# Plot accuracy\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(best_fold_train_acc) + 1), best_fold_train_acc, label='Train Accuracy', color='blue')\nplt.plot(range(1, len(best_fold_test_acc) + 1), best_fold_test_acc, label='Test Accuracy', color='orange')\nplt.title('Accuracy vs Epochs of Wavelet Energy (Valence)')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy (Kernel 5x5)')\nplt.legend()\n\n# Plot loss\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(best_fold_losses) + 1), best_fold_losses, label='Loss', color='red')\nplt.title('Loss vs Epochs of Wavelet Energy (Valence)')\nplt.xlabel('Epochs')\nplt.ylabel('Loss (Kernel 5x5)')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('accuracy_loss_Wavelet_Energy_valence_5ker.png')\nplt.show()\n\n# Confusion Matrix for best fold\ndef plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title(title)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.savefig('confusion_matrix_Wavelet_Energy_valence_5ker.png')\n    plt.show()\n\nplot_confusion_matrix(best_y_true, best_y_pred, title='Confusion Matrix of Wavelet Energy (Valence) with Kernel 5x5')\n\n# Classification Report for best fold\nprint('Classification Report of Wavelet Energy (Valence) with Kernel 5x5:')\nprint(classification_report(best_y_true, best_y_pred))\n\n# ROC Curve for best fold\ndef plot_roc_curve(y_true, y_scores):\n    fpr, tpr, _ = roc_curve(y_true, y_scores)\n    roc_auc = roc_auc_score(y_true, y_scores)\n    np.savetxt('roc_curve_Wavelet_Energy_valence_5ker.txt', np.c_[fpr, tpr], header='FPR TPR', comments='')\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) of Wavelet Energy with Kernel 5x5 (Valence)')\n    plt.legend(loc='lower right')\n    plt.savefig('roc_curve_Wavelet_Energy_valence_5ker.png')\n    plt.show()\n\nplot_roc_curve(best_y_true, best_y_scores)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
